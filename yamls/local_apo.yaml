seed: 4667

max_seq_len: 10240
model:
  name: hf_critic_free_lm
  pretrained: true
  use_auth_token: true
  loss_type: apo
  beta: 1e-3 # TODO: need to
  normalize_advantage: true
  length_normalize_policy_loss: true
  policy_clip_ratio: 0.2
  compute_kl_loss: true
  target_kl: 0.1
  kl_estimator: k3
  kl_clip_range: 40.0
  use_flash_attention_2: true
  pretrained_model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct

#loggers:
#  mlflow:
#    tags:
#      run: apo_test
#      project: apo
#    tracking_uri: databricks
#    experiment_name: null  # TODO: add mlflow experiment name

callbacks:
  on_policy_rl: {}
  lr_monitor: {}
  memory_monitor: {}

optimizer:
  lr: 7.0e-07
  name: decoupled_adamw
  betas:
  - 0.9
  - 0.95
  weight_decay: 0

precision: amp_bf16
scheduler:
  name: cosine_with_warmup
  alpha_f: 0.01
  t_warmup: 10iter

tokenizer:
  name: ${variables.tokenizer_name}
  kwargs:
    padding: longest
    pad_token: <|endoftext|>
    truncation: true
    padding_side: left
    model_max_length: ${max_seq_len}
    trust_remote_code: true

variables:
  tokenizer_name: Qwen/Qwen2.5-1.5B-Instruct

  reference_model:
    precision: amp_bf16
    pretrained: true
    model_config:
      name: hf_causal_lm
      pretrained: true
      use_auth_token: true
      use_flash_attention_2: true
      pretrained_model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct

  kl_controller: # Turn off reward KL
    kl_ctl_type: fixed
    init_kl_coef: 0.0

  # The non-train-FSDP config
  non_train_fsdp_config:
    sharding_strategy: SHARD_GRAD_OP
    mixed_precision: DEFAULT
    activation_checkpointing: true
    activation_cpu_offload: false
    verbose: false
    limit_all_gathers: true
    state_dict_type: sharded
    use_orig_params: true

  generation_kwargs:
    top_p: 1
    top_k: 0
    do_sample: true
    use_cache: true
    temperature: 1

  gamma: 1
  lambda_gae: 1

  num_batches_per_update: 4
  generations_per_prompt: 1
  device_generate_batch_size: 1
  epoch_per_iteration: 1

  buffer:
    name: MinibatchRolloutBuffer
    max_buffer_size: ${variables.num_batches_per_update}

  rewards:
    bad_generation_end:
      reward: -1
      eos_penalty: true
      reward_type: bad_generation_end
    math_verifier:
      reward_type: math_verifier
      reward: 4
    math_format_verifier:
      reward_type: math_format_verifier
      reward: 1

  global_seed: 17
  max_gen_len: 32
  eos_token_ids:
  - 151643
  - 151645


algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0e-02

autoresume: true
save_folder: null  # TODO: fill this in
save_overwrite: true
save_interval: 25iter
save_num_checkpoints_to_keep: 1

fsdp_config:
  verbose: false
  cpu_offload: false
  mixed_precision: PURE
  state_dict_type: sharded
  use_orig_params: true
  forward_prefetch: true
  backward_prefetch: BACKWARD_PRE
  sharding_strategy: FULL_SHARD
  activation_checkpointing: true
  activation_cpu_offload: false
  activation_checkpointing_reentrant: false

train_loader:
  name: prompt
  dataset:
    local: /tmp/dataset/prompt/
    split: train
    # remote:
    shuffle: true
    max_gen_len: ${variables.max_gen_len}
    max_seq_len: ${max_seq_len}
    shuffle_seed: ${variables.global_seed}
    download_timeout: 3600
  drop_last: true
  num_workers: 8

log_config: true
dist_timeout: 3600
progress_bar: false
eval_interval: 25iter
max_duration: 1000iter
log_to_console: true
python_log_level: debug
console_log_interval: 1ba
global_train_batch_size: 8
device_train_microbatch_size: 1
